{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Tracing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this module, we've taken a look at the traceable decorator, and how we can use it to set up tracing.\n",
    "\n",
    "In this lesson, we're going to look at alternative ways in which we can set up tracing, and when you should think about using these different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain and LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using LangChain or LangGraph, all we need to do to set up tracing is to set a few environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"LangSmithRAG/1.0 (Python; Ollama)\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about our graph implementation here, you can learn more about LangGraph through our LangGraph Academy course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import operator\n",
    "from langchain.schema import Document\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# Define Graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes\n",
    "def retrieve_documents(state: GraphState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
    "\n",
    "# Define Graph\n",
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "display(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're setting up a simple graph in LangGraph. If you want to learn more about LangGraph, I would highly recommend taking a look at our LangGraph Academy course.\n",
    "\n",
    "You can also pass in metadata or other fields through an optional config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"How do I set up tracing if I'm using LangChain?\",\n",
       " 'messages': [HumanMessage(content=\"How do I set up tracing if I'm using LangChain?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"To set up tracing with LangChain, you need to set the LANGCHAIN_TRACING_V2 environment variable to 'true' and the LANGCHAIN_API_KEY environment variable to your API key. Additionally, you can configure other settings such as LANGCHAIN_CALLBACKS_BACKGROUND to reduce latency.\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-08T20:19:04.337052892Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1984111903, 'load_duration': 1348127955, 'prompt_eval_count': 989, 'prompt_eval_duration': 246000000, 'eval_count': 57, 'eval_duration': 388000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-522cc4c4-20c9-4915-acb2-30824dea866d-0', usage_metadata={'input_tokens': 989, 'output_tokens': 57, 'total_tokens': 1046})],\n",
       " 'documents': [Document(metadata={'id': 'a750e943-01b4-493e-9f34-039d890a713a', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain'}, page_content='Trace with LangChain (Python and JS/TS) | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       "  Document(metadata={'id': '27a73200-96e6-4efd-b340-f1f82729d88d', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing'}, page_content=\"noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section.\"),\n",
       "  Document(metadata={'id': 'da70aa91-a5f4-4143-a41d-486bb6682f15', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph'}, page_content=\"Without LangChain\\u200b\\nIf you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately\\n(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).\\nIf you do so, LangSmith will automatically nest traces from those wrapped methods.\\nHere's an example. You can also see this page for more information.\\n1. Installation\\u200b\\nInstall the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).\\npipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph\\n2. Configure your environment\\u200b\\nPythonTypeScriptexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\\nIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\\nexport LANGCHAIN_CALLBACKS_BACKGROUND=false\\nSee this LangChain.js guide for more information.\"),\n",
       "  Document(metadata={'id': 'f09e6ab9-2018-4ce5-9b42-aa5d56b6f3e6', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides'}, page_content=\"Trace with LangChain\\nTrace with LangGraph\\nTrace the OpenAI API client\\nTrace with Instructor (Python only)\\nTrace with the Vercel AI SDK (JS only)\\nTrace with OpenTelemetry\\n\\nAdvanced configuration\\u200b\\n\\nConfigure threads\\nSet a sampling rate for traces\\nAdd metadata and tags to traces\\nImplement distributed tracing\\nAccess the current span within a traced function\\nLog multimodal traces\\nLog retriever traces\\nLog custom LLM traces / provide custom token counts\\nPrevent logging of sensitive data in traces\\nTrace generator functions\\nCalculate token-based costs for traces\\nTrace JS functions in serverless environments\\nTroubleshoot trace testing\\nUpload files with traces\\nPrint out logs from the LangSmith SDK (Python Only)\\nTroubleshooting: Missing or Misrouted Traces\\n\\nTracing projects UI & API\\u200b\\nView and interact with your traces to debug your applications.\\n\\nFilter traces in a project\\nSave a filter for your project\\nQuery / Export traces using the SDK (low volume)\\nBulk exporting traces (high volume)\\nShare or unshare a trace publicly\\nCompare traces\\nView threads\\n\\nDashboards\\u200b\\nUse LangSmith custom and built-in dashboards to gain insight into your production systems.\\n\\nCreate and use custom dashboards\\nUse built-in monitoring dashboards\\n\\nAutomations\\u200b\\nLeverage LangSmith's powerful monitoring, automation, and online evaluation features to make sense of your production data.\\n\\nSet up automation rules\\nSet up webhook notifications for rules\\nPerform online evaluations\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousAdd observability to your LLM applicationNextAnnotate code for tracingTracing configurationBasic configurationTrace natively supported librariesAdvanced configurationTracing projects UI & APIDashboardsAutomationsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\")]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I set up tracing if I'm using LangChain?\"\n",
    "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\n",
    "\n",
    "You want to log traces for a specific block of code.\n",
    "You want control over the inputs, outputs, and other attributes of the trace.\n",
    "It is not feasible to use a decorator or wrapper.\n",
    "Any or all of the above.\n",
    "The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application.\n",
    "\n",
    "You still need to set your `LANGCHAIN_API_KEY` and `LANGCHAIN_TRACING_V2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable, trace\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"ollama\"\n",
    "MODEL_NAME = \"llama3.2\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",\n",
    ")\n",
    "\n",
    "\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/chat\"\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    return documents\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    with trace(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"formatted_docs\": formatted_docs},\n",
    "        metadata={\"foo\": \"bar\"},\n",
    "    ) as ls_trace:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "        ]\n",
    "        response = call_openai(messages)\n",
    "        ls_trace.end(outputs={\"output\": response})\n",
    "        return response\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output using OpenAI client or Ollama native API\n",
    "\"\"\"\n",
    "@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict],\n",
    "    model: str = MODEL_NAME,\n",
    "    temperature: float = 0.0\n",
    ") -> str:\n",
    "    try:\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        response = requests.post(OLLAMA_API_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the tracing context, you should import the `tracing_context` from the `langsmith.run_helpers` module and then wrap your code within it. The example provided shows how to use this context manager in a server application. You can also use the `traceable` decorator on functions or classes to enable tracing automatically.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with tracing context?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrap_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application.\n",
    "\n",
    "You still need to set your `LANGCHAIN_API_KEY` and `LANGCHAIN_TRACING_V2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import wrap_openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "import openai\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"ollama\"\n",
    "MODEL_NAME = \"llama3.2:latest\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Wrap the OpenAI Client\n",
    "openai_client = wrap_openai(\n",
    "    openai.Client(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\"\n",
    "    )\n",
    ")\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    # TODO: We don't need to use @traceable on a nested function call anymore,\n",
    "    # wrap_openai takes care of this for us\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict],\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag_with_wrap_openai(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the `wrap_openai` method in Python or TypeScript, you need to import it from \"langsmith/traceable\" and then call it with the OpenAI client instance, like so:\n",
      "\n",
      "```python\n",
      "import { wrap_openai } from 'langsmith/traceable';\n",
      "const openai = new OpenAI(); // Create an instance of the OpenAI client\n",
      "const wrappedClient = wrap_openai(openai); // Wrap the OpenAI client for tracing\n",
      "```\n",
      "\n",
      "In TypeScript, you would use `wrapOpenAI` instead:\n",
      "\n",
      "```typescript\n",
      "import { wrapOpenAI } from 'langsmith/traceable';\n",
      "const openai = new OpenAI(); // Create an instance of the OpenAI client\n",
      "const wrappedClient = wrapOpenAI(openai); // Wrap the OpenAI client for tracing\n",
      "```\n",
      "\n",
      "After wrapping, you can use your wrapped client instance as before.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with wrap_openai?\"\n",
    "ai_answer = langsmith_rag_with_wrap_openai(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapped OpenAI client accepts all the same langsmith_extra parameters as @traceable decorated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-312', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The color of the sky can vary depending on the time of day, atmospheric conditions, and other factors. Here are some common colors of the sky:\\n\\n1. **Blue**: During a clear day, the sky typically appears blue due to a phenomenon called Rayleigh scattering, where shorter wavelengths of light (like blue and violet) are scattered more than longer wavelengths (like red and orange).\\n2. **Sunrise and sunset**: During these times, the sky can take on hues of warm colors like orange, pink, red, and purple, depending on the amount of dust and water vapor in the atmosphere.\\n3. **Cloudy skies**: On overcast days, the sky can appear gray or white due to the scattering of light by cloud particles.\\n4. **Twilight**: During twilight hours (just before sunrise or after sunset), the sky can take on a range of colors including pink, orange, purple, and blue.\\n5. **At night**: The sky can appear dark blue or black, depending on the presence of stars and moonlight.\\n\\nRemember that these are general descriptions, and the actual color of the sky can vary from one location to another and even from day to day!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741465148, model='llama3.2:latest', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=240, prompt_tokens=31, total_tokens=271, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What color is the sky?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    langsmith_extra={\"metadata\": {\"foo\": \"bar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Advanced] RunTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGCHAIN_API_KEY`, but `LANGCHAIN_TRACING_V2` is not necessary for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-academy\"\n",
    "os.environ[\"USER_AGENT\"] = \"LangSmithRAG/1.0 (Python; Ollama)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# I have my env variables defined in a .env file\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and set `LANGCHAIN_TRACING_V2` to false, as we are using RunTree to manually create runs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled() # This should return false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have rewritten our RAG application, except this time we pass a RunTree argument through our function calls, and create child runs at each layer. This gives our RunTree the same hierarchy that we were automatically able to establish with @traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import RunTree\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama',  # requis mais non utilis√© par Ollama\n",
    ")\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "def retrieve_documents(parent_run: RunTree, question: str):\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Retrieve Documents\",\n",
    "        run_type=\"retriever\",\n",
    "        inputs={\"question\": question},\n",
    "    )\n",
    "    documents = retriever.invoke(question)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"documents\": documents})\n",
    "    child_run.post()\n",
    "    return documents\n",
    "\n",
    "def generate_response(parent_run: RunTree, question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"documents\": documents},\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    openai_response = call_openai(child_run, messages)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"openai_response\": openai_response})\n",
    "    child_run.post()\n",
    "    return openai_response\n",
    "\n",
    "def call_openai(\n",
    "    parent_run: RunTree, messages: List[dict], model: str = \"llama3.2\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"OpenAI Call\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": messages},\n",
    "    )\n",
    "    openai_response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"openai_response\": openai_response})\n",
    "    child_run.post()\n",
    "    return openai_response.choices[0].message.content  # Ajout√© pour retourner directement le contenu\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    # Create a root RunTree\n",
    "    root_run_tree = RunTree(\n",
    "        name=\"Chat Pipeline\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question}\n",
    "    )\n",
    "\n",
    "    # Pass our RunTree into the nested function calls\n",
    "    documents = retrieve_documents(root_run_tree, question)\n",
    "    response = generate_response(root_run_tree, question, documents)\n",
    "    output = response  # Modifi√© car call_openai retourne d√©j√† le contenu\n",
    "\n",
    "    # Post our final output\n",
    "    root_run_tree.end(outputs={\"generation\": output})\n",
    "    root_run_tree.post()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the RunTree API, you need to create a RunTree object from the request headers and then pass it to the `withRunTree` function along with your traced function. The `withRunTree` function will execute your traced function within the context of the RunTree, allowing you to access tracing metadata such as the current run (span) and its children.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with RunTree?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
