{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using filters in the SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `list_runs` method in the SDK or `/runs/query` endpoint in the API, you can filter runs to analyze and export."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's paste in the raw query we copied from the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_query = \"\"\"eq(name, \"call_openai\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=UUID('aaf58a59-a798-4846-bc84-ae289d39ebf5') name='call_openai' start_time=datetime.datetime(2025, 3, 17, 10, 15, 11, 774620) run_type='llm' end_time=datetime.datetime(2025, 3, 17, 10, 15, 27, 946128) extra={'metadata': {'ls_method': 'traceable', 'ls_model_name': 'llama3.2', 'ls_provider': 'ollama', 'revision_id': '299dd4b-dirty'}, 'runtime': {'langchain_core_version': '0.3.41', 'langchain_version': '0.3.20', 'library': 'langsmith', 'platform': 'Linux-6.8.0-47-generic-x86_64-with-glibc2.39', 'py_implementation': 'CPython', 'runtime': 'python', 'runtime_version': '3.11.11', 'sdk': 'langsmith-py', 'sdk_version': '0.3.15'}} error=None serialized=None events=[] inputs={'messages': [{'content': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\nWhenerver possbile provide a python code example to help the user to get started!\", 'role': 'system'}, {'content': \"Context: Update the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesAnnotate code for tracingOn this pageAnnotate code for tracing\\nnoteIf you've decided you no longer want to trace your runs, you can remove the LANGSMITH_TRACING environment variable.\\nNote that this does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable\\u200b\\n\\n3. Log a trace\\u200b\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nUpdating the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!\\n\\n Question: How do I set up tracing to LangSmith with @traceable?\", 'role': 'user'}]} outputs={'id': 'chatcmpl-994', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'To set up tracing to LangSmith with @traceable, you can wrap or decorate your custom functions using the `@traceable` decorator. \\n\\nHere\\'s a simple example in Python:\\n\\n```python\\nfrom @traceable import traceable\\n\\n@traceable\\ndef my_function(example_string: str) -> str:\\n    # code that needs to be traced\\n    return \"Processed string: \" + example_string\\n```\\n\\nNote that `my_function` is now wrapped with the `@traceable` decorator, which enables tracing for this function.', 'role': 'assistant'}}], 'created': 1742206527, 'model': 'llama3.2', 'object': 'chat.completion', 'system_fingerprint': 'fp_ollama', 'usage': {'completion_tokens': 112, 'prompt_tokens': 694, 'total_tokens': 806}} reference_example_id=None parent_run_id=UUID('7211b597-6c5b-4b42-b90b-0b0cbb9d4ad5') tags=[] attachments={} session_id=UUID('7815772b-63c8-4c3b-ae79-9cba994243a6') child_run_ids=None child_runs=None feedback_stats=None app_path='/o/7506181e-8a5f-454a-bc4d-ab4006c3ead0/projects/p/7815772b-63c8-4c3b-ae79-9cba994243a6/r/aaf58a59-a798-4846-bc84-ae289d39ebf5?trace_id=e76d5307-0365-41ee-9460-cdf3033e2dbb&start_time=2025-03-17T10:15:11.452780' manifest_id=None status='success' prompt_tokens=694 completion_tokens=112 total_tokens=806 first_token_time=None total_cost=None prompt_cost=None completion_cost=None parent_run_ids=[UUID('e76d5307-0365-41ee-9460-cdf3033e2dbb'), UUID('7211b597-6c5b-4b42-b90b-0b0cbb9d4ad5')] trace_id=UUID('e76d5307-0365-41ee-9460-cdf3033e2dbb') dotted_order='20250317T101511452780Ze76d5307-0365-41ee-9460-cdf3033e2dbb.20250317T101511676061Z7211b597-6c5b-4b42-b90b-0b0cbb9d4ad5.20250317T101511774620Zaaf58a59-a798-4846-bc84-ae289d39ebf5' in_dataset=False\n",
      "id=UUID('9cc12ccc-e0c8-4468-9900-c2fb81eb50fc') name='call_openai' start_time=datetime.datetime(2025, 3, 17, 10, 12, 6, 822685) run_type='llm' end_time=datetime.datetime(2025, 3, 17, 10, 12, 19, 323157) extra={'metadata': {'ls_method': 'traceable', 'ls_model_name': 'llama3.2', 'ls_provider': 'ollama', 'revision_id': '299dd4b-dirty'}, 'runtime': {'langchain_core_version': '0.3.41', 'langchain_version': '0.3.20', 'library': 'langsmith', 'platform': 'Linux-6.8.0-47-generic-x86_64-with-glibc2.39', 'py_implementation': 'CPython', 'runtime': 'python', 'runtime_version': '3.11.11', 'sdk': 'langsmith-py', 'sdk_version': '0.3.15'}} error=None serialized=None events=[] inputs={'messages': [{'role': 'system', 'content': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\"}, {'role': 'user', 'content': \"Context: Update the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesAnnotate code for tracingOn this pageAnnotate code for tracing\\nnoteIf you've decided you no longer want to trace your runs, you can remove the LANGSMITH_TRACING environment variable.\\nNote that this does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable\\u200b\\n\\n3. Log a trace\\u200b\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nUpdating the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt! \\n\\n Question: How do I set up tracing to LangSmith with @traceable?\"}]} outputs={'id': 'chatcmpl-821', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': \"To set up tracing to LangSmith with @traceable, you should use the `@traceable` decorator and configure your application to track spans. Update your code to include `import @traceable from '@traceable/core';`. This setup will enable the tracable tracer for your Langsmith implementation.\", 'role': 'assistant'}}], 'created': 1742206339, 'model': 'llama3.2', 'object': 'chat.completion', 'system_fingerprint': 'fp_ollama', 'usage': {'completion_tokens': 63, 'prompt_tokens': 676, 'total_tokens': 739}} reference_example_id=None parent_run_id=UUID('6c1f6a21-46ec-466c-8124-cb8a413272ef') tags=[] attachments={} session_id=UUID('7815772b-63c8-4c3b-ae79-9cba994243a6') child_run_ids=None child_runs=None feedback_stats=None app_path='/o/7506181e-8a5f-454a-bc4d-ab4006c3ead0/projects/p/7815772b-63c8-4c3b-ae79-9cba994243a6/r/9cc12ccc-e0c8-4468-9900-c2fb81eb50fc?trace_id=5459dc48-29ac-44f9-a57f-be38d890443c&start_time=2025-03-17T10:12:05.635218' manifest_id=None status='success' prompt_tokens=676 completion_tokens=63 total_tokens=739 first_token_time=None total_cost=None prompt_cost=None completion_cost=None parent_run_ids=[UUID('5459dc48-29ac-44f9-a57f-be38d890443c'), UUID('6c1f6a21-46ec-466c-8124-cb8a413272ef')] trace_id=UUID('5459dc48-29ac-44f9-a57f-be38d890443c') dotted_order='20250317T101205635218Z5459dc48-29ac-44f9-a57f-be38d890443c.20250317T101206822348Z6c1f6a21-46ec-466c-8124-cb8a413272ef.20250317T101206822685Z9cc12ccc-e0c8-4468-9900-c2fb81eb50fc' in_dataset=False\n",
      "id=UUID('40250f86-95a9-431b-9b66-146110a686e7') name='call_openai' start_time=datetime.datetime(2025, 3, 17, 9, 52, 44, 856302) run_type='llm' end_time=datetime.datetime(2025, 3, 17, 9, 52, 57, 310818) extra={'metadata': {'ls_method': 'traceable', 'ls_model_name': 'llama3.2', 'ls_provider': 'ollama', 'revision_id': '299dd4b-dirty'}, 'runtime': {'langchain_core_version': '0.3.41', 'langchain_version': '0.3.20', 'library': 'langsmith', 'platform': 'Linux-6.8.0-47-generic-x86_64-with-glibc2.39', 'py_implementation': 'CPython', 'runtime': 'python', 'runtime_version': '3.11.11', 'sdk': 'langsmith-py', 'sdk_version': '0.3.15'}} error=None serialized=None events=[] inputs={'messages': [{'role': 'system', 'content': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\"}, {'role': 'user', 'content': \"Context: Update the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesAnnotate code for tracingOn this pageAnnotate code for tracing\\nnoteIf you've decided you no longer want to trace your runs, you can remove the LANGSMITH_TRACING environment variable.\\nNote that this does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable\\u200b\\n\\n3. Log a trace\\u200b\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nUpdating the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt! \\n\\n Question: How do I set up tracing to LangSmith with @traceable?\"}]} outputs={'id': 'chatcmpl-972', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'To set up tracing to LangSmith with @traceable, you need to wrap or decorate the custom functions/SDKs you want to trace. Once setup, LangSmith will infer the proper tracing config. You can also remove LANGSMITH_TRACING environment variable if you no longer want to trace your runs.', 'role': 'assistant'}}], 'created': 1742205177, 'model': 'llama3.2', 'object': 'chat.completion', 'system_fingerprint': 'fp_ollama', 'usage': {'completion_tokens': 63, 'prompt_tokens': 676, 'total_tokens': 739}} reference_example_id=None parent_run_id=UUID('48408306-f852-4fb3-8af3-6085b140a6ca') tags=[] attachments={} session_id=UUID('7815772b-63c8-4c3b-ae79-9cba994243a6') child_run_ids=None child_runs=None feedback_stats=None app_path='/o/7506181e-8a5f-454a-bc4d-ab4006c3ead0/projects/p/7815772b-63c8-4c3b-ae79-9cba994243a6/r/40250f86-95a9-431b-9b66-146110a686e7?trace_id=52d83a07-2d54-47e5-ad72-1b8a010d0eed&start_time=2025-03-17T09:52:44.631291' manifest_id=None status='success' prompt_tokens=676 completion_tokens=63 total_tokens=739 first_token_time=None total_cost=None prompt_cost=None completion_cost=None parent_run_ids=[UUID('52d83a07-2d54-47e5-ad72-1b8a010d0eed'), UUID('48408306-f852-4fb3-8af3-6085b140a6ca')] trace_id=UUID('52d83a07-2d54-47e5-ad72-1b8a010d0eed') dotted_order='20250317T095244631291Z52d83a07-2d54-47e5-ad72-1b8a010d0eed.20250317T095244855895Z48408306-f852-4fb3-8af3-6085b140a6ca.20250317T095244856302Z40250f86-95a9-431b-9b66-146110a686e7' in_dataset=False\n",
      "id=UUID('be03fa8e-1b8f-4c57-921b-d9d9a7371870') name='call_openai' start_time=datetime.datetime(2025, 3, 17, 9, 51, 41, 329823) run_type='llm' end_time=datetime.datetime(2025, 3, 17, 9, 51, 54, 99844) extra={'metadata': {'ls_method': 'traceable', 'ls_model_name': 'llama3.2', 'ls_provider': 'ollama', 'revision_id': '299dd4b-dirty'}, 'runtime': {'langchain_core_version': '0.3.41', 'langchain_version': '0.3.20', 'library': 'langsmith', 'platform': 'Linux-6.8.0-47-generic-x86_64-with-glibc2.39', 'py_implementation': 'CPython', 'runtime': 'python', 'runtime_version': '3.11.11', 'sdk': 'langsmith-py', 'sdk_version': '0.3.15'}} error=None serialized=None events=[] inputs={'messages': [{'role': 'system', 'content': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\"}, {'role': 'user', 'content': \"Context: Update the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesAnnotate code for tracingOn this pageAnnotate code for tracing\\nnoteIf you've decided you no longer want to trace your runs, you can remove the LANGSMITH_TRACING environment variable.\\nNote that this does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable\\u200b\\n\\n3. Log a trace\\u200b\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nUpdating the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt! \\n\\n Question: How do I set up tracing to LangSmith with @traceable?\"}]} outputs={'id': 'chatcmpl-863', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': \"To set up tracing to LangSmith with @traceable, wrap or decorate your custom functions/SDKs you want to trace using the `@traceable` decorator. This will enable LangSmith to infer the proper tracing configuration for your application. You'll also need to configure your logging settings and update your environment variables as needed.\", 'role': 'assistant'}}], 'created': 1742205114, 'model': 'llama3.2', 'object': 'chat.completion', 'system_fingerprint': 'fp_ollama', 'usage': {'completion_tokens': 67, 'prompt_tokens': 676, 'total_tokens': 743}} reference_example_id=None parent_run_id=UUID('9ff9a000-a5d4-470b-804d-f68b174b04fc') tags=[] attachments={} session_id=UUID('7815772b-63c8-4c3b-ae79-9cba994243a6') child_run_ids=None child_runs=None feedback_stats=None app_path='/o/7506181e-8a5f-454a-bc4d-ab4006c3ead0/projects/p/7815772b-63c8-4c3b-ae79-9cba994243a6/r/be03fa8e-1b8f-4c57-921b-d9d9a7371870?trace_id=db87e85f-c3ed-4eeb-9e22-a90c625553d6&start_time=2025-03-17T09:51:41.113018' manifest_id=None status='success' prompt_tokens=676 completion_tokens=67 total_tokens=743 first_token_time=None total_cost=None prompt_cost=None completion_cost=None parent_run_ids=[UUID('db87e85f-c3ed-4eeb-9e22-a90c625553d6'), UUID('9ff9a000-a5d4-470b-804d-f68b174b04fc')] trace_id=UUID('db87e85f-c3ed-4eeb-9e22-a90c625553d6') dotted_order='20250317T095141113018Zdb87e85f-c3ed-4eeb-9e22-a90c625553d6.20250317T095141329515Z9ff9a000-a5d4-470b-804d-f68b174b04fc.20250317T095141329823Zbe03fa8e-1b8f-4c57-921b-d9d9a7371870' in_dataset=False\n",
      "id=UUID('3f82d9c9-fcae-46cf-be8e-19d3ee2572d8') name='call_openai' start_time=datetime.datetime(2025, 3, 17, 9, 50, 49, 899561) run_type='llm' end_time=datetime.datetime(2025, 3, 17, 9, 50, 49, 905483) extra={'metadata': {'ls_method': 'traceable', 'ls_model_name': 'gpt-4o-mini', 'ls_provider': 'openai', 'revision_id': '299dd4b-dirty'}, 'runtime': {'langchain_core_version': '0.3.41', 'langchain_version': '0.3.20', 'library': 'langsmith', 'platform': 'Linux-6.8.0-47-generic-x86_64-with-glibc2.39', 'py_implementation': 'CPython', 'runtime': 'python', 'runtime_version': '3.11.11', 'sdk': 'langsmith-py', 'sdk_version': '0.3.15'}} error='NotFoundError(\\'Error code: 404 - {\\\\\\'error\\\\\\': {\\\\\\'message\\\\\\': \\\\\\'model \"gpt-4o-mini\" not found, try pulling it first\\\\\\', \\\\\\'type\\\\\\': \\\\\\'api_error\\\\\\', \\\\\\'param\\\\\\': None, \\\\\\'code\\\\\\': None}}\\')\\n\\nTraceback (most recent call last):\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 879, in create\\n    return self._post(\\n           ^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_base_client.py\", line 1296, in post\\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_base_client.py\", line 973, in request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_base_client.py\", line 1077, in _request\\n    raise self._make_status_error_from_response(err.response) from None\\nopenai.NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \\'model \"gpt-4o-mini\" not found, try pulling it first\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}\\n' serialized=None events=[] inputs={'messages': [{'role': 'system', 'content': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\"}, {'role': 'user', 'content': \"Context: Update the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesAnnotate code for tracingOn this pageAnnotate code for tracing\\nnoteIf you've decided you no longer want to trace your runs, you can remove the LANGSMITH_TRACING environment variable.\\nNote that this does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable\\u200b\\n\\n3. Log a trace\\u200b\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nUpdating the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt! \\n\\n Question: How do I set up tracing to LangSmith with @traceable?\"}]} outputs={'output': None} reference_example_id=None parent_run_id=UUID('9261848c-72bd-45e0-a995-d3ede1c4efb7') tags=[] attachments={} session_id=UUID('7815772b-63c8-4c3b-ae79-9cba994243a6') child_run_ids=None child_runs=None feedback_stats=None app_path='/o/7506181e-8a5f-454a-bc4d-ab4006c3ead0/projects/p/7815772b-63c8-4c3b-ae79-9cba994243a6/r/3f82d9c9-fcae-46cf-be8e-19d3ee2572d8?trace_id=7005c51e-463d-41b7-a456-9276781e94d7&start_time=2025-03-17T09:50:49.677926' manifest_id=None status='error' prompt_tokens=0 completion_tokens=0 total_tokens=0 first_token_time=None total_cost=Decimal('0.0') prompt_cost=Decimal('0.0') completion_cost=Decimal('0.0') parent_run_ids=[UUID('7005c51e-463d-41b7-a456-9276781e94d7'), UUID('9261848c-72bd-45e0-a995-d3ede1c4efb7')] trace_id=UUID('7005c51e-463d-41b7-a456-9276781e94d7') dotted_order='20250317T095049677926Z7005c51e-463d-41b7-a456-9276781e94d7.20250317T095049899009Z9261848c-72bd-45e0-a995-d3ede1c4efb7.20250317T095049899561Z3f82d9c9-fcae-46cf-be8e-19d3ee2572d8' in_dataset=False\n",
      "id=UUID('8f80ce9c-2658-4a8c-8fab-29fb4383faa9') name='call_openai' start_time=datetime.datetime(2025, 3, 17, 9, 50, 15, 322523) run_type='llm' end_time=datetime.datetime(2025, 3, 17, 9, 50, 15, 333129) extra={'metadata': {'ls_method': 'traceable', 'ls_model_name': 'gpt-4o-mini', 'ls_provider': 'openai', 'revision_id': '299dd4b-dirty'}, 'runtime': {'langchain_core_version': '0.3.41', 'langchain_version': '0.3.20', 'library': 'langsmith', 'platform': 'Linux-6.8.0-47-generic-x86_64-with-glibc2.39', 'py_implementation': 'CPython', 'runtime': 'python', 'runtime_version': '3.11.11', 'sdk': 'langsmith-py', 'sdk_version': '0.3.15'}} error='NotFoundError(\\'Error code: 404 - {\\\\\\'error\\\\\\': {\\\\\\'message\\\\\\': \\\\\\'model \"gpt-4o-mini\" not found, try pulling it first\\\\\\', \\\\\\'type\\\\\\': \\\\\\'api_error\\\\\\', \\\\\\'param\\\\\\': None, \\\\\\'code\\\\\\': None}}\\')\\n\\nTraceback (most recent call last):\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 879, in create\\n    return self._post(\\n           ^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_base_client.py\", line 1296, in post\\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_base_client.py\", line 973, in request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n  File \"/home/pi/miniconda3/envs/pocmg/lib/python3.11/site-packages/openai/_base_client.py\", line 1077, in _request\\n    raise self._make_status_error_from_response(err.response) from None\\nopenai.NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \\'model \"gpt-4o-mini\" not found, try pulling it first\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}\\n' serialized=None events=[] inputs={'messages': [{'role': 'system', 'content': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\"}, {'role': 'user', 'content': \"Context: Update the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt!\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesAnnotate code for tracingOn this pageAnnotate code for tracing\\nnoteIf you've decided you no longer want to trace your runs, you can remove the LANGSMITH_TRACING environment variable.\\nNote that this does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable\\u200b\\n\\n3. Log a trace\\u200b\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nUpdating the application\\u200b\\nWe can now update our code to pull down the dataset we are sending runs to.\\nOnce we pull it down, we can create a string with the examples in it.\\nWe can then put this string as part of the prompt! \\n\\n Question: How do I set up tracing to LangSmith with @traceable?\"}]} outputs={'output': None} reference_example_id=None parent_run_id=UUID('8b801ef2-aa1c-4e7c-b0f2-af0404e9eea6') tags=[] attachments={} session_id=UUID('7815772b-63c8-4c3b-ae79-9cba994243a6') child_run_ids=None child_runs=None feedback_stats=None app_path='/o/7506181e-8a5f-454a-bc4d-ab4006c3ead0/projects/p/7815772b-63c8-4c3b-ae79-9cba994243a6/r/8f80ce9c-2658-4a8c-8fab-29fb4383faa9?trace_id=07af76b1-79a6-421e-84ce-bb0fec4138d0&start_time=2025-03-17T09:50:14.118596' manifest_id=None status='error' prompt_tokens=0 completion_tokens=0 total_tokens=0 first_token_time=None total_cost=Decimal('0.0') prompt_cost=Decimal('0.0') completion_cost=Decimal('0.0') parent_run_ids=[UUID('07af76b1-79a6-421e-84ce-bb0fec4138d0'), UUID('8b801ef2-aa1c-4e7c-b0f2-af0404e9eea6')] trace_id=UUID('07af76b1-79a6-421e-84ce-bb0fec4138d0') dotted_order='20250317T095014118596Z07af76b1-79a6-421e-84ce-bb0fec4138d0.20250317T095015321596Z8b801ef2-aa1c-4e7c-b0f2-af0404e9eea6.20250317T095015322523Z8f80ce9c-2658-4a8c-8fab-29fb4383faa9' in_dataset=False\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "client = Client()\n",
    "runs = client.list_runs(\n",
    "  project_name=\"langsmith-academy\", \n",
    "  filter=raw_query,\n",
    "  start_time=datetime.now() - timedelta(days=1),\n",
    ")\n",
    "for run in runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in trace or tree filters too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_runs(\n",
    "  project_name=\"langsmith-academy\", \n",
    "  filter=\"eq(is_root, true)\",\n",
    "  # trace_filter=\"\"\n",
    "  # tree_filter=\"\"\n",
    ")\n",
    "for run in runs:\n",
    "    print(run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
