{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple RAG](../../images/simple_rag.png)\n",
    "\n",
    "In this notebook, we're going to set up a simple RAG application that we'll be using as we learn more about LangSmith.\n",
    "\n",
    "RAG (Retrieval Augmented Generation) is a popular technique for providing LLMs with relevant documents that will enable them to better answer questions from users. \n",
    "\n",
    "In our case, we are going to index some LangSmith documentation!\n",
    "\n",
    "LangSmith makes it easy to trace any LLM application, no LangChain required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you set your environment variables, including your Langsmith API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline!\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.5.13\n"
     ]
    }
   ],
   "source": [
    "!ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to \"What is the capital of France?\" is Paris.\n",
      "LangSmith appears to be a programming language, although its primary use isn't explicitly stated in the provided context.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "import requests\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "import os\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "# Set USER_AGENT to avoid warning\n",
    "os.environ[\"USER_AGENT\"] = \"LangSmithRAG/1.0 (Python; Ollama)\"\n",
    "\n",
    "MODEL_PROVIDER = \"ollama\"\n",
    "MODEL_NAME = \"llama3.2:latest\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the retrieved context to answer the question in one concise sentence, focusing only on the answer itself. \n",
    "If the context lacks sufficient information, say \"I don't have enough context to answer accurately\" and stop there.\"\"\"\n",
    "\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_ollama` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs}\\n\\nQuestion: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
    "    return call_ollama(prompt)\n",
    "\n",
    "\"\"\"\n",
    "call_ollama\n",
    "- Returns the text generation output from Ollama (renamed for structure compatibility)\n",
    "\"\"\"\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_ollama(\n",
    "    messages: str,\n",
    "    model: str = MODEL_NAME,\n",
    "    temperature: float = 0.0\n",
    ") -> str:\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_API_URL, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str, **kwargs):\n",
    "    documents = retrieve_documents(question)\n",
    "    # print(f\"Retrieved documents: {[doc.page_content for doc in documents]}\")\n",
    "    response = generate_response(question, documents)\n",
    "    # print(f\"Generated response: {response}\")\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # First question\n",
    "    question1 = \"What is the capital of France?\"\n",
    "    answer1 = langsmith_rag(question1)\n",
    "    print(answer1)\n",
    "\n",
    "    # Second question with metadata\n",
    "    question2 = \"What is LangSmith used for?\"\n",
    "    answer2 = langsmith_rag(question2, langsmith_extra={\"metadata\": {\"website\": \"www.google.com\"}})\n",
    "    print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should take a little less than a minute. We are indexing and storing LangSmith documentation in a SKLearn vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith appears to be a programming language or development tool, but its specific use case and purpose are not clearly defined in the provided context.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangSmith used for?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"website\": \"www.google.com\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
